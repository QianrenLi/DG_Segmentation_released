{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo for project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import load_dataset\n",
    "\n",
    "trainset, valset = load_dataset(train=True,is_vert_flip = False,is_rotate = True,is_translate = True,is_color_jitter = False,is_DG=False)\n",
    "testset = load_dataset(train=False)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    # The data need to be normalized and unnormalized to keep the same\n",
    "    # img = img / 255\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, targets = next(dataiter)\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print(names)\n",
    "for i in range(batch_size):\n",
    "    target_np = torch.argmax(targets[i], dim=0).cpu().numpy()\n",
    "    plt.subplot(1,batch_size,i+1)\n",
    "    \n",
    "    plt.imshow(target_np,cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Domain generalization example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import domain_generization, load_dataset\n",
    "# trainset_temp, _ = load_dataset(train=True,is_vert_flip = True,is_rotate = True,is_translate = True,is_color_jitter = False,is_DG=False)\n",
    "trainset_temp, _ = load_dataset(train=True,is_vert_flip = False,is_rotate = False,is_translate = False,is_color_jitter = False,is_DG=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "original_image, _ = trainset_temp.__getitem__(0) # Use the first image in training set.\n",
    "original_image = original_image / 2 + 0.5\n",
    "original_image = original_image.numpy()\n",
    "scaling_factor = 0.3 # 替换低频区域所占大小\n",
    "ratio = 1 #替换区域中目标域图片的幅度比重 \n",
    "num_generalized=10\n",
    "domains = ['domain1','domain2','domain3'] # 可选：'random'（默认随机选择domain）, 'domain1', 'domain2', 'domain3'.\n",
    "original_image_freq = np.fft.fftshift(np.fft.fft2(original_image,axes=(-2,-1)),axes=(-2,-1))\n",
    "original_image_freq = np.log(np.abs(original_image_freq))/np.max(np.log(np.abs(original_image_freq))) # log and normalize\n",
    "plt.figure(figsize=[4,2])\n",
    "plt.subplot(1,2,1), plt.imshow(np.transpose(original_image, (1, 2, 0))), plt.xlabel('Raw Image.')\n",
    "plt.subplot(1,2,2), plt.imshow(np.transpose(original_image_freq, (1, 2, 0))), plt.xlabel('Raw Image (frequency).')\n",
    "dg_outputs_domains = []\n",
    "for domain in domains:\n",
    "    dg_outputs, dg_fre_outputs= np.array(domain_generization(original_image,scaling_factor, ratio,num_generalized,domain)) # 输出是一个float, 因为计算傅里叶变换的时候应该用float提高精度\n",
    "    dg_outputs = np.real(dg_outputs)\n",
    "    fig, axs = plt.subplots(2, num_generalized, figsize=(2*num_generalized, 2*2), layout=\"constrained\")\n",
    "    for i in range(num_generalized):\n",
    "        dg_output = dg_outputs[i]\n",
    "        dg_fre_output = dg_fre_outputs[i]\n",
    "        dg_fre_output = np.log(np.abs(dg_fre_output))/np.max(np.log(np.abs(dg_fre_output))) # log and normalize\n",
    "        axs[0,i].imshow(np.clip(np.transpose(dg_output, (1, 2, 0)),0,1)) # 转回int才可以直接imshow\n",
    "        axs[0,i].set_xlabel('After DG (image).')\n",
    "        axs[1,i].imshow(np.clip(np.transpose(dg_fre_output,(1,2,0)),0,1),cmap='gray')\n",
    "        axs[1,i].set_xlabel('After DG (frequency).')\n",
    "    plt.suptitle('Domain generalization example: {}.'.format(domain))\n",
    "    dg_outputs_domains.append(dg_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dis_rep import norm_dist,CS_dist,intra_cluster_distance,inter_cluster_diatance\n",
    "D = np.zeros((3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i == j:\n",
    "            D[i,j] = intra_cluster_distance(dg_outputs_domains[i],distance_metric=norm_dist,ord=None)\n",
    "        else:\n",
    "            D[i,j] = inter_cluster_diatance(dg_outputs_domains[i],dg_outputs_domains[j],distance_type=2,distance_metric=norm_dist,ord=None)[0]\n",
    "print('Distances between 3 domains:')\n",
    "print(D)\n",
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[0,:,:], rgb[1,:,:], rgb[2,:,:] # PIL读到图片是RGB顺序\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray\n",
    "domain1_gray_average = np.mean(rgb2gray(dg_outputs_domains[0]),axis=0)\n",
    "domain2_gray_average = np.mean(rgb2gray(dg_outputs_domains[1]),axis=0)\n",
    "domain3_gray_average = np.mean(rgb2gray(dg_outputs_domains[2]),axis=0)\n",
    "histogram1, bin_edges1 = np.histogram(domain1_gray_average, bins=256, range=(-0.5, 0.5))\n",
    "histogram2, bin_edges2 = np.histogram(domain2_gray_average, bins=256, range=(-0.5, 0.5))\n",
    "histogram3, bin_edges3 = np.histogram(domain3_gray_average, bins=256, range=(-0.5, 0.5))\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Grayscale Histograms\")\n",
    "plt.xlabel(\"grayscale value\")\n",
    "plt.ylabel(\"pixel count\")\n",
    "plt.plot(bin_edges1[0:-1], histogram1,'r', bin_edges2[0:-1], histogram2, 'g', bin_edges3[0:-1], histogram3, 'b')\n",
    "plt.figlegend([\"domain1\",\"domain2\",\"domain3\"],loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self,weight=None,size_average = True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "    def forward(self,inputs,targets,smooth=1):\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        batch_number = targets.size(0)\n",
    "        # inputs = inputs[:,0,:,:]\n",
    "        Dice_BCE = 0\n",
    "        for i in range(2):\n",
    "            a = inputs[:,i].view(batch_number,-1)\n",
    "            b = targets[:,i].view(batch_number,-1)\n",
    "            intersection = (a*b).sum()\n",
    "            dice_loss = 1 - (2. * intersection + smooth) / (a.sum() + b.sum() + smooth)\n",
    "            Dice_BCE += dice_loss\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = Dice_BCE + BCE\n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "net = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=2,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "criterion = DiceBCELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "net = net.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 177875.49it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 115704.94it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 209820.11it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 186662.39it/s]\n",
      "/home/lqr11812417/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 1.589\n",
      "Finished Training\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 208154.04it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 196730.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 207536.07it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 197008.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 2.033\n",
      "Finished Training\n",
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 153468.86it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 153862.95it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 214432.72it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 94636.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5159/1777402650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self,weight=None,size_average = True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "    def forward(self,inputs,targets,smooth=1):\n",
    "        inputs = F.sigmoid(inputs)\n",
    "        batch_number = targets.size(0)\n",
    "        # inputs = inputs[:,0,:,:]\n",
    "        Dice_BCE = 0\n",
    "        for i in range(2):\n",
    "            a = inputs[:,i].view(batch_number,-1)\n",
    "            b = targets[:,i].view(batch_number,-1)\n",
    "            intersection = (a*b).sum()\n",
    "            dice_loss = 1 - (2. * intersection + smooth) / (a.sum() + b.sum() + smooth)\n",
    "            Dice_BCE += dice_loss\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = Dice_BCE + BCE\n",
    "        return Dice_BCE\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import load_dataset\n",
    "import time\n",
    "\n",
    "epoch_number = 1\n",
    "\n",
    "for train_idx in range(4):\n",
    "\n",
    "    #################### Model Define ######################\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import segmentation_models_pytorch as smp\n",
    "    import torch\n",
    "\n",
    "    net = smp.Unet(\n",
    "        encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "        encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "        in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "        classes=2,                      # model output channels (number of classes in your dataset)\n",
    "    )\n",
    "\n",
    "    import torch.optim as optim\n",
    "    import torch.nn as nn\n",
    "\n",
    "\n",
    "    criterion = DiceBCELoss()\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    net = net.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    #################### Model Define ######################\n",
    "\n",
    "    if train_idx == 0:\n",
    "        ############# No DG #################\n",
    "        trainset, valset = load_dataset(train=True,is_vert_flip = False,is_rotate = False,is_translate = False,is_color_jitter = False,is_DG=False)\n",
    "        PATH = ('./net_no_DG_epoch_%d.pth') % (epoch_number)\n",
    "        log_file_name = ('./no_DG_epoch_%d.txt')%(epoch_number)\n",
    "        ############# No DG #################\n",
    "    elif train_idx == 1:\n",
    "        # ############## DG #################\n",
    "        trainset, valset = load_dataset(train=True,is_vert_flip = False,is_rotate = False,is_translate = False,is_color_jitter = False,is_DG=True)\n",
    "        PATH = ('./net_DG_epoch_%d.pth') % (epoch_number)\n",
    "        log_file_name = ('./DG_epoch_%d.txt')%(epoch_number)\n",
    "        # ############## DG #################\n",
    "    elif train_idx == 2:\n",
    "        ############## Data argument + No DG #################\n",
    "        trainset, valset = load_dataset(train=True,is_vert_flip = True,is_rotate = True,is_translate = True,is_color_jitter = False,is_DG=True)\n",
    "        PATH = ('./net_Data_arg_no_DG_epoch_%d.pth') % (epoch_number)\n",
    "        log_file_name = ('./Data_arg_no_DG_epoch_%d.txt')%(epoch_number)\n",
    "        ############## Data argument + No DG #################\n",
    "    elif train_idx == 3:\n",
    "        # ############ Data argument +  DG #################\n",
    "        trainset, valset = load_dataset(train=True,is_vert_flip = True,is_rotate = True,is_translate = True,is_color_jitter = False,is_DG=True)\n",
    "        PATH = ('./net_Data_arg_DG_epoch_%d.pth') % (epoch_number)\n",
    "        log_file_name = ('./Data_arg_DG_epoch_%d.txt')%(epoch_number)\n",
    "        # ############## Data argument +  DG #################\n",
    "    else:\n",
    "        print(\"outof model bound\")\n",
    "        break\n",
    "    \n",
    "    # trainset, valset = load_dataset(train=True,is_vert_flip = True,is_rotate = True,is_translate = True,is_color_jitter = False,is_DG=False)\n",
    "    # trainset, valset = load_dataset(train=True,is_DG=False)\n",
    "\n",
    "    log_file = open(log_file_name,'w')\n",
    "\n",
    "    ########### Delete file content\n",
    "    log_file.seek(0)\n",
    "    log_file.truncate()\n",
    "    ########### Delete file content\n",
    "\n",
    "    testset = load_dataset(train=False)\n",
    "\n",
    "    batch_size = 4\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "    for epoch in range(epoch_number):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, target) in enumerate(trainloader):\n",
    "            \n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            # forward + backward + optimizeo \n",
    "            # change the data to float type\n",
    "            outputs = net(inputs)\n",
    "            # print(outputs.size())\n",
    "            # outputs = torch.argmax(outputs, dim=1)\n",
    "            # print((outputs.shape))\n",
    "            # print(target.to(torch.float32).dtype)\n",
    "\n",
    "            ############## Cross entropy Loss ##########\n",
    "            # loss = criterion(outputs,target)\n",
    "            #################### End ###################\n",
    "            \n",
    "            ############## Use dice loss ###############\n",
    "            # outputs = torch.argmax(outputs, dim=1)\n",
    "            # target = torch.argmax(target, dim=1)\n",
    "            loss = criterion(outputs,target)\n",
    "            # loss.requires_grad_(True)\n",
    "            #################### End ###################\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            log_file.write((\"%f\\n\") % loss.item())\n",
    "            # print((\"%f\\n\") % loss.item())\n",
    "            if i % 20 == 19:    # print every 20 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f}')\n",
    "                running_loss = 0.0\n",
    "    log_file.close()\n",
    "    ################## Temp Model Save ##############\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "Note the validation set is highly related with the training set, therefore it is not wise to generate a new validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dscs = []\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        images, targets = data\n",
    "        ############ Activate this for CUDA ##################\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        ############ Activate this for CUDA ##################\n",
    "        outputs = net(images.to(torch.float32))\n",
    "        \n",
    "        inputs = F.sigmoid(outputs)\n",
    "        batch_number = targets.size(0)\n",
    "        # inputs = inputs[:,0,:,:]\n",
    "        smooth = 1\n",
    "        Dice_BCE = 0\n",
    "        for j in range(2):\n",
    "            a = inputs[:,j].view(batch_number,-1)\n",
    "            b = targets[:,j].view(batch_number,-1)\n",
    "            intersection = (a*b).sum()\n",
    "            dice_loss = 1 - (2. * intersection + smooth) / (a.sum() + b.sum() + smooth)\n",
    "            Dice_BCE += dice_loss\n",
    "\n",
    "        dscs.append(1 - Dice_BCE) \n",
    "dsc_test = np.mean(dscs)\n",
    "print(('Dsc of test set %d: %f')% (i,dsc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file.close()\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from dataset import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from medpy import metric\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nclasses = 2\n",
    "############## No DG #################\n",
    "PATH = './net_no_DG.pth'\n",
    "# PATH = './net_DG.pth'\n",
    "# PATH = './net_Data_arg_no_DG.pth'\n",
    "# PATH = './net_Data_arg_DG.pth'\n",
    "############## No DG #################\n",
    "\n",
    "############## Metric Type ################\n",
    "metric_type = \"DICE\"\n",
    "# metric_type = \"HD95\"\n",
    "############## Metric Type ################\n",
    "\n",
    "net = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=nclasses,                      # model output channels (number of classes in your dataset)\n",
    ")\n",
    "############ Activate this for CUDA ##################\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = net.to(device)\n",
    "############ Activate this for CUDA ##################\n",
    "\n",
    "\n",
    "\n",
    "net.load_state_dict(torch.load(PATH,map_location='cpu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    # Contruct test loader for different test region\n",
    "    test_data_str = (\"./data/Pro1-SegmentationData/Domain%d/data/*.bmp\") % (i + 1)\n",
    "    test_label_str = (\"./data/Pro1-SegmentationData/Domain%d/label/{}.bmp\") % (i + 1)\n",
    "    if i == 0:\n",
    "        test_data_str = (\"./data/Pro1-SegmentationData/Domain%d/data/*.jpg\") % (i + 1)\n",
    "        test_label_str = (\"./data/Pro1-SegmentationData/Domain%d/label/{}.png\") % (i + 1)\n",
    "    testset = load_dataset(train=False,test_data_str = test_data_str, test_label_str = test_label_str)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "    dscs = []\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, targets, names = data\n",
    "            ############ Activate this for CUDA ##################\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            ############ Activate this for CUDA ##################\n",
    "            outputs = net(images.to(torch.float32))\n",
    "            \n",
    "            if metric_type == \"DICE\":\n",
    "                ############ DICE metric #############################\n",
    "                inputs = F.sigmoid(outputs)\n",
    "                batch_number = targets.size(0)\n",
    "                # inputs = inputs[:,0,:,:]\n",
    "                smooth = 1\n",
    "                Dice_BCE = 0\n",
    "                for j in range(2):\n",
    "                    a = inputs[:,j].view(batch_number,-1)\n",
    "                    b = targets[:,j].view(batch_number,-1)\n",
    "                    intersection = (a*b).sum()\n",
    "                    dice_loss = 1 - (2. * intersection + smooth) / (a.sum() + b.sum() + smooth)\n",
    "                    Dice_BCE += dice_loss\n",
    "\n",
    "                dscs.append(1 - Dice_BCE) \n",
    "                ############ DICE metric #############################\n",
    "            elif metric_type ==  \"HD95\":\n",
    "                ############ HD 95 metric ############################\n",
    "                for idx, name in enumerate(names):\n",
    "                    output_np = torch.argmax(outputs[idx], dim=0).cpu().numpy()\n",
    "                    binary_output = np.array(output_np)\n",
    "                    target_np = torch.argmax(targets[idx], dim=0).cpu().numpy()\n",
    "                    # target_1d = np.reshape(target_np, (-1, 1))\n",
    "                    # pred_1d = np.reshape(binary_output, (-1, 1))\n",
    "                    # plt.imshow(output_np,cmap='gray')\n",
    "                    # plt.show()\n",
    "                    # dsc = max(directed_hausdorff(output_np,target_np)[0],directed_hausdorff(target_np,output_np)[0])\n",
    "                    dsc = metric.binary.hd95(target_np,output_np)\n",
    "                    dscs.append(dsc) \n",
    "                ############ HD 95 metric ############################\n",
    "            else:\n",
    "                print((\"Not a valid/implemented metric %s\") % metric_type)\n",
    "                break\n",
    "            ############ Dice by TA ##############################\n",
    "            # for idx, name in enumerate(names):\n",
    "            #     output_np = torch.argmax(outputs[idx], dim=0).cpu().numpy()\n",
    "            #     binary_output = np.array(output_np)\n",
    "            #     target_np = torch.argmax(targets[idx], dim=0).cpu().numpy()\n",
    "            #     target_1d = np.reshape(target_np, (-1, 1))\n",
    "            #     pred_1d = np.reshape(binary_output, (-1, 1))\n",
    "            #     # plt.imshow(output_np,cmap='gray')\n",
    "            #     # plt.show()\n",
    "            #     accuracy = accuracy_score(target_1d, pred_1d)\n",
    "            #     # print(accuracy)\n",
    "            #     if nclasses == 2:\n",
    "            #         dsc = f1_score(target_1d, pred_1d) # f1_score就是Dice\n",
    "            #     else:\n",
    "            #         dsc = f1_score(target_1d, pred_1d,average='micro')\n",
    "            #     dscs.append(dsc) \n",
    "            ############ Dice by TA ##############################\n",
    "\n",
    "    dsc_test = np.mean(dscs)\n",
    "    print(('Dsc of test set %d: %f')% (i + 1,dsc_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show last predicted result\n",
    "import matplotlib.pyplot as plt\n",
    "for i,_ in enumerate(images):\n",
    "    image = images.cpu().numpy()[i]\n",
    "    target_np = torch.argmax(targets[i], dim=0).cpu().numpy()\n",
    "    output_np = torch.argmax(outputs[i], dim=0).cpu().numpy()\n",
    "    image = image/2 + 0.5\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(np.transpose(image,(1,2,0)))\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(target_np,cmap='gray')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(output_np,cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff00b26d6b990ba17d2f20ddcdd7d48aa18f7559c6b1d8ae74680a301a112ff1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
